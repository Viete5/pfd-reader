"""
–°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã StudyMate
"""

import asyncio
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from evaluation.evaluate_integrated import IntegratedEvaluator
from evaluation.evaluate_rag_metrics import RAGEvaluator
from evaluation.evaluate_concept_agents import ConceptAgentEvaluator
from evaluation.evaluate_system_metrics import SystemMetricsEvaluator


def print_banner():
    """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Å–∏–≤—ã–π –±–∞–Ω–Ω–µ—Ä"""
    banner = """
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                STUDY-MATE AGENT SYSTEM EVALUATION        ‚ïë
    ‚ïë                                                          ‚ïë
    ‚ïë  üîç RAG Metrics    üß† Concept Agents   ‚ö° System Metrics ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)


def load_test_data() -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    test_data_path = os.path.join(current_dir, "evaluation", "test_data", "test_queries.json")

    if os.path.exists(test_data_path):
        with open(test_data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print(f"‚ö†Ô∏è  –§–∞–π–ª —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_data_path}")
        print("   –ò—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        return {
            "rag_test_queries": [
                {
                    "query": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ–∏–∑–∏–∫–∞?",
                    "relevant_docs": ["–§–∏–∑–∏–∫–∞ - –Ω–∞—É–∫–∞ –æ –ø—Ä–∏—Ä–æ–¥–µ, –∏–∑—É—á–∞—é—â–∞—è —Å–≤–æ–π—Å—Ç–≤–∞ –º–∞—Ç–µ—Ä–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏."]
                }
            ],
            "performance_test_queries": [
                "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è?",
                "–û–±—ä—è—Å–Ω–∏ –∑–∞–∫–æ–Ω –û–º–∞",
                "–ù–∞–π–¥–∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ"
            ]
        }


async def evaluate_rag_only(user_id: int = 12345) -> Dict:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫—É RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("\nüîç –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 50)

    evaluator = RAGEvaluator()
    test_data = load_test_data()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    from src.tools.pdf_indexer import get_user_db_path
    db_path = get_user_db_path(user_id)

    if not os.path.exists(db_path):
        print("‚ö†Ô∏è  –£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
        print("   –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RAG —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   python test_rag.py")
        return {"error": "No documents loaded"}

    results = {}

    # –û—Ü–µ–Ω–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    if "rag_test_queries" in test_data:
        print("\n1. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞...")
        retrieval_metrics = await evaluator.evaluate_retrieval_quality(
            user_id, test_data["rag_test_queries"][:3]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
        results["retrieval"] = retrieval_metrics

        print("   Precision@3: {:.3f}".format(retrieval_metrics.get("precision@3_mean", 0)))
        print("   Recall@3: {:.3f}".format(retrieval_metrics.get("recall@3_mean", 0)))
        print("   MRR: {:.3f}".format(retrieval_metrics.get("mrr_mean", 0)))

    # –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤
    if "rag_test_cases" in test_data:
        print("\n2. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ LLM...")
        response_metrics = await evaluator.evaluate_response_quality(
            user_id, test_data["rag_test_cases"][:2]
        )
        results["response"] = response_metrics

        print("   Relevance Score: {:.3f}".format(response_metrics.get("avg_relevance", 0)))
        print("   Factuality Score: {:.3f}".format(response_metrics.get("avg_factuality", 0)))
        print("   Success Rate: {:.1%}".format(response_metrics.get("success_rate", 0)))

    return results


async def evaluate_concepts_only() -> Dict:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫—É –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
    print("\nüß† –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò –ê–ì–ï–ù–¢–û–í –ö–û–ù–¶–ï–ü–¢–û–í")
    print("=" * 50)

    from evaluation.evaluate_concept_agents import run_concept_agent_evaluation
    results = await run_concept_agent_evaluation()
    return results


async def evaluate_system_only(user_id: int = 12345) -> Dict:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–∏—Å—Ç–µ–º–Ω—É—é –æ—Ü–µ–Ω–∫—É"""
    print("\n‚ö° –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–ù–û–ô –û–¶–ï–ù–ö–ò")
    print("=" * 50)

    evaluator = SystemMetricsEvaluator()
    test_data = load_test_data()

    if "performance_test_queries" in test_data:
        queries = test_data["performance_test_queries"][:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    else:
        queries = ["–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ–∏–∑–∏–∫–∞?", "–û–±—ä—è—Å–Ω–∏ –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—é"]

    results = await evaluator.run_comprehensive_evaluation(user_id)
    return results


async def evaluate_full_system(user_id: int = 12345) -> Dict:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å–∏—Å—Ç–µ–º—ã"""
    print("\nüéØ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ô –û–¶–ï–ù–ö–ò –°–ò–°–¢–ï–ú–´")
    print("=" * 50)

    evaluator = IntegratedEvaluator()
    results = await evaluator.run_full_evaluation(user_id)
    return results


def save_results(results: Dict, filename: str = "evaluation_results.json"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª"""
    results_dir = os.path.join(current_dir, "results")
    os.makedirs(results_dir, exist_ok=True)

    filepath = os.path.join(results_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filepath}")
    return filepath


def print_quick_summary(results: Dict, evaluation_type: str):
    """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print(f"\nüìä –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï ({evaluation_type}):")
    print("-" * 40)

    if evaluation_type == "RAG" and "retrieval" in results:
        rag = results.get("retrieval", {})
        print(f"Precision@3: {rag.get('precision@3_mean', 0):.3f}")
        print(f"Recall@3: {rag.get('recall@3_mean', 0):.3f}")
        print(f"MRR: {rag.get('mrr_mean', 0):.3f}")

    elif evaluation_type == "Concepts" and "concept_explanation" in results:
        exp = results.get("concept_explanation", {})
        print(f"Clarity: {exp.get('avg_clarity', 0):.3f}")
        print(f"Completeness: {exp.get('avg_completeness', 0):.3f}")
        print(f"Structure: {exp.get('avg_structure', 0):.3f}")

    elif evaluation_type == "System" and "summary" in results:
        summary = results.get("summary", {})
        print(f"Performance: {summary.get('performance_score', 0):.1f}/100")
        print(f"Reliability: {summary.get('reliability_score', 0):.1f}/100")
        print(f"Overall: {summary.get('overall_score', 0):.1f}/100")

    elif evaluation_type == "Full" and "overall_score" in results:
        print(f"Overall System Score: {results.get('overall_score', 0):.1f}/100")
        for component, score in results.get('scores', {}).items():
            print(f"{component}: {score:.1f}/100")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã StudyMate')
    parser.add_argument('--type', type=str, default='full',
                        choices=['rag', 'concepts', 'system', 'full', 'all'],
                        help='–¢–∏–ø –æ—Ü–µ–Ω–∫–∏: rag, concepts, system, full, all')
    parser.add_argument('--user-id', type=int, default=12345,
                        help='ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--output', type=str, default=None,
                        help='–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--quick', action='store_true',
                        help='–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ (–º–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤)')

    args = parser.parse_args()

    print_banner()
    print(f"\n‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞:")
    print(f"   –¢–∏–ø –æ—Ü–µ–Ω–∫–∏: {args.type}")
    print(f"   User ID: {args.user_id}")
    print(f"   –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: {'–î–∞' if args.quick else '–ù–µ—Ç'}")

    all_results = {}

    try:
        if args.type in ['rag', 'all']:
            rag_results = await evaluate_rag_only(args.user_id)
            all_results['rag'] = rag_results
            print_quick_summary(rag_results, "RAG")

        if args.type in ['concepts', 'all']:
            concept_results = await evaluate_concepts_only()
            all_results['concepts'] = concept_results
            print_quick_summary(concept_results, "Concepts")

        if args.type in ['system', 'all']:
            system_results = await evaluate_system_only(args.user_id)
            all_results['system'] = system_results
            print_quick_summary(system_results, "System")

        if args.type in ['full', 'all']:
            full_results = await evaluate_full_system(args.user_id)
            all_results['full'] = full_results
            print_quick_summary(full_results, "Full")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if all_results:
            filename = args.output or f"evaluation_{args.type}_{args.user_id}.json"
            saved_path = save_results(all_results, filename)

            print(f"\nüéâ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")
            print(f"üìÑ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª: {os.path.basename(saved_path)}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã
            results_dir = os.path.join(current_dir, "results")
            if os.path.exists(results_dir):
                files = os.listdir(results_dir)
                if files:
                    print(f"\nüìà –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—Ç—á–µ—Ç–æ–≤:")
                    for file in files:
                        if file.endswith(('.png', '.csv', '.txt')):
                            print(f"   - {file}")

            print(f"\nüìã –î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
            print(f"   1. –û—Ç–∫—Ä–æ–π—Ç–µ –ø–∞–ø–∫—É 'results/'")
            print(f"   2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ evaluation_report.png –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
            print(f"   3. –û—Ç–∫—Ä–æ–π—Ç–µ final_evaluation_report.txt –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞")

        else:
            print("\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  –û—Ü–µ–Ω–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
