import asyncio
import time
import psutil
import pandas as pd
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor
import numpy as np


class SystemMetricsEvaluator:
    def __init__(self):
        self.metrics_history = []

    async def measure_performance(self, user_id: int, queries: List[str]) -> Dict:
        """
        –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã
        """
        from src.core.orchestrator import handle_user_query

        results = []

        for query in queries:
            try:
                # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

                response = await handle_user_query(user_id, query)

                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024

                execution_time = end_time - start_time
                memory_delta = end_memory - start_memory

                results.append({
                    "query": query[:50] + "..." if len(query) > 50 else query,
                    "execution_time": execution_time,
                    "memory_usage_mb": end_memory,
                    "memory_delta_mb": memory_delta,
                    "response_length": len(response),
                    "success": True
                })

            except Exception as e:
                results.append({
                    "query": query[:50] + "...",
                    "error": str(e),
                    "execution_time": 0,
                    "success": False
                })

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if results:
            df = pd.DataFrame([r for r in results if r["success"]])

            if not df.empty:
                return {
                    "avg_execution_time": df["execution_time"].mean(),
                    "max_execution_time": df["execution_time"].max(),
                    "min_execution_time": df["execution_time"].min(),
                    "throughput_qps": 1 / df["execution_time"].mean() if df["execution_time"].mean() > 0 else 0,
                    "avg_memory_usage_mb": df["memory_usage_mb"].mean(),
                    "avg_memory_delta_mb": df["memory_delta_mb"].mean(),
                    "success_rate": len(df) / len(results),
                    "total_queries": len(results)
                }

        return {"error": "No successful queries"}

    async def load_test(self, user_id: int, num_concurrent: int = 10) -> Dict:
        """
        –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
        """
        from src.core.orchestrator import handle_user_query

        test_queries = [
                           "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ–∏–∑–∏–∫–∞?",
                           "–û–±—ä—è—Å–Ω–∏ –∑–∞–∫–æ–Ω –û–º–∞",
                           "–ù–∞–π–¥–∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ",
                           "–î–∞–π —É—á–µ–±–Ω—ã–µ —Å–æ–≤–µ—Ç—ã",
                           "–ö–∞–∫ —É–ª—É—á—à–∏—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç—ã?"
                       ] * (num_concurrent // 2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤

        async def run_query(query):
            try:
                start_time = time.time()
                await handle_user_query(user_id, query)
                return time.time() - start_time
            except Exception as e:
                return None

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        start_time = time.time()
        tasks = [run_query(query) for query in test_queries[:num_concurrent]]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful_times = [r for r in results if isinstance(r, (int, float))]

        return {
            "total_time_seconds": total_time,
            "queries_per_second": len(successful_times) / total_time if total_time > 0 else 0,
            "avg_response_time": np.mean(successful_times) if successful_times else 0,
            "p95_response_time": np.percentile(successful_times, 95) if successful_times else 0,
            "success_rate": len(successful_times) / len(results) if results else 0,
            "concurrent_users": num_concurrent,
            "total_queries": len(results)
        }

    async def evaluate_scalability(self, user_ids: List[int]) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã
        """
        test_query = "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è?"
        results = []

        for num_users in [1, 3, 5, 10]:
            start_time = time.time()

            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            tasks = []
            for i in range(min(num_users, len(user_ids))):
                user_id = user_ids[i]
                tasks.append(self._single_user_query(user_id, test_query))

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            await asyncio.gather(*tasks)
            total_time = time.time() - start_time

            results.append({
                "num_users": num_users,
                "total_time": total_time,
                "throughput": num_users / total_time if total_time > 0 else 0
            })

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
        df = pd.DataFrame(results)
        scalability_factor = df["throughput"].iloc[-1] / df["throughput"].iloc[0] if len(df) > 1 else 1

        return {
            "scalability_results": results,
            "scalability_factor": scalability_factor,
            "is_linear_scaling": scalability_factor > 0.7 * (df["num_users"].iloc[-1] / df["num_users"].iloc[0])
        }

    async def _single_user_query(self, user_id: int, query: str):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        from src.core.orchestrator import handle_user_query
        try:
            await handle_user_query(user_id, query)
        except:
            pass

    def collect_resource_metrics(self) -> Dict:
        """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"""
        process = psutil.Process()

        return {
            "cpu_percent": process.cpu_percent(interval=1),
            "memory_percent": process.memory_percent(),
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "threads_count": process.num_threads(),
            "open_files": len(process.open_files()),
            "disk_io": process.io_counters()
        }

    async def run_comprehensive_evaluation(self, user_id: int = 12345):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""

        print("‚ö° –ù–∞—á–∏–Ω–∞—é —Å–∏—Å—Ç–µ–º–Ω—É—é –æ—Ü–µ–Ω–∫—É...")

        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_queries = [
            "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ–∏–∑–∏–∫–∞?",
            "–û–±—ä—è—Å–Ω–∏ –≤—Ç–æ—Ä–æ–π –∑–∞–∫–æ–Ω –ù—å—é—Ç–æ–Ω–∞",
            "–ù–∞–π–¥–∏ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ",
            "–ö–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—á–∏—Ç—å—Å—è?",
            "–£–ª—É—á—à–∏ –º–æ–π –∫–æ–Ω—Å–ø–µ–∫—Ç",
            "–°–æ–∑–¥–∞–π —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω –Ω–∞ –Ω–µ–¥–µ–ª—é"
        ]

        # 1. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print("\n1. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        perf_metrics = await self.measure_performance(user_id, test_queries)
        for key, value in perf_metrics.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.3f}")
            else:
                print(f"   {key}: {value}")

        # 2. –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("\n2. –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (10 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π):")
        load_metrics = await self.load_test(user_id, num_concurrent=10)
        for key, value in load_metrics.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.3f}")
            else:
                print(f"   {key}: {value}")

        # 3. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
        print("\n3. –û—Ü–µ–Ω–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏:")
        user_ids = [12345, 12346, 12347, 12348, 12349]
        scalability_metrics = await self.evaluate_scalability(user_ids)
        print(f"   –§–∞–∫—Ç–æ—Ä –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏: {scalability_metrics['scalability_factor']:.3f}")
        print(f"   –õ–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: {scalability_metrics['is_linear_scaling']}")

        # 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
        print("\n4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤:")
        resource_metrics = self.collect_resource_metrics()
        for key, value in resource_metrics.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.2f}")
            elif hasattr(value, '_asdict'):
                print(f"   {key}: {value._asdict()}")
            else:
                print(f"   {key}: {value}")

        # –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        print("\nüìä –°–í–û–î–ù–´–ô –û–¢–ß–ï–¢:")
        summary = {
            "performance_score": self._calculate_performance_score(perf_metrics),
            "scalability_score": scalability_metrics["scalability_factor"] * 10,
            "reliability_score": load_metrics.get("success_rate", 0) * 100,
            "resource_efficiency": 100 - resource_metrics.get("memory_percent", 0)
        }

        for key, value in summary.items():
            print(f"   {key}: {value:.1f}/100")

        overall_score = np.mean(list(summary.values()))
        print(f"\nüéØ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –°–ò–°–¢–ï–ú–´: {overall_score:.1f}/100")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        self._generate_recommendations(summary)

        return {
            "performance": perf_metrics,
            "load_test": load_metrics,
            "scalability": scalability_metrics,
            "resources": resource_metrics,
            "summary": summary,
            "overall_score": overall_score
        }

    def _calculate_performance_score(self, perf_metrics: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        score = 100

        # –®—Ç—Ä–∞—Ñ –∑–∞ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if perf_metrics.get("avg_execution_time", 10) > 5:
            score -= 30
        elif perf_metrics.get("avg_execution_time", 10) > 2:
            score -= 15

        # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        if perf_metrics.get("throughput_qps", 0) > 1:
            score += 10

        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        if perf_metrics.get("avg_memory_usage_mb", 1000) > 500:
            score -= 20

        return max(0, min(100, score))

    def _generate_recommendations(self, summary: Dict):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")

        recommendations = []

        if summary["performance_score"] < 70:
            recommendations.append("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ RAG —Å–∏—Å—Ç–µ–º—ã")
            recommendations.append("‚Ä¢ –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã")

        if summary["scalability_score"] < 70:
            recommendations.append("‚Ä¢ –£–ª—É—á—à–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤")
            recommendations.append("‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—É–ª–æ–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")

        if summary["resource_efficiency"] < 70:
            recommendations.append("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏")
            recommendations.append("‚Ä¢ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—á–∏—Å—Ç–∫—É –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å–µ—Å—Å–∏–π")

        if not recommendations:
            recommendations.append("‚Ä¢ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ! –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏")

        for rec in recommendations:
            print(f"   {rec}")


async def main():
    evaluator = SystemMetricsEvaluator()
    results = await evaluator.run_comprehensive_evaluation()
    return results


if __name__ == "__main__":
    asyncio.run(main())
