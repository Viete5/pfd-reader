import asyncio
import numpy as np
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import pandas as pd
from src.tools.rag_query import RAGLoader


class RAGEvaluator:
    def __init__(self, embedding_model: str = "cointegrated/rubert-tiny2"):
        self.embedding_model = SentenceTransformer(embedding_model)

    async def evaluate_retrieval_quality(self, user_id: int, test_queries: List[Dict]) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        """
        try:
            loader = RAGLoader(user_id)
            retriever = loader.retriever

            metrics = {
                "precision_at_k": [],
                "recall_at_k": [],
                "mrr": [],
                "hit_rate": []
            }

            for query_data in test_queries:
                query = query_data["query"]
                relevant_docs = query_data["relevant_docs"]

                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
                retrieved_docs = retriever.get_relevant_documents(query)
                retrieved_texts = [doc.page_content for doc in retrieved_docs]

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                precision_k = self._calculate_precision_at_k(retrieved_texts, relevant_docs, k=3)
                recall_k = self._calculate_recall_at_k(retrieved_texts, relevant_docs, k=3)
                mrr = self._calculate_mrr(retrieved_texts, relevant_docs)
                hit_rate = self._calculate_hit_rate(retrieved_texts, relevant_docs)

                metrics["precision_at_k"].append(precision_k)
                metrics["recall_at_k"].append(recall_k)
                metrics["mrr"].append(mrr)
                metrics["hit_rate"].append(hit_rate)

            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            return {
                "precision@3_mean": np.mean(metrics["precision_at_k"]),
                "precision@3_std": np.std(metrics["precision_at_k"]),
                "recall@3_mean": np.mean(metrics["recall_at_k"]),
                "recall@3_std": np.std(metrics["recall_at_k"]),
                "mrr_mean": np.mean(metrics["mrr"]),
                "hit_rate_mean": np.mean(metrics["hit_rate"]),
                "num_queries": len(test_queries)
            }

        except Exception as e:
            return {"error": str(e)}

    def _calculate_precision_at_k(self, retrieved: List[str], relevant: List[str], k: int = 3) -> float:
        """Precision@K - —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–≤—ã—Ö K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        retrieved_k = retrieved[:k]
        if not retrieved_k:
            return 0.0

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤–º–µ—Å—Ç–æ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        relevant_count = 0
        for ret_doc in retrieved_k:
            if any(self._semantic_similarity(ret_doc, rel_doc) > 0.6 for rel_doc in relevant):
                relevant_count += 1

        return relevant_count / k

    def _calculate_recall_at_k(self, retrieved: List[str], relevant: List[str], k: int = 3) -> float:
        """Recall@K - –ø–æ–ª–Ω–æ—Ç–∞ –ø–µ—Ä–≤—ã—Ö K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        retrieved_k = retrieved[:k]
        if not relevant:
            return 0.0

        # –ù–∞—Ö–æ–¥–∏–º, —Å–∫–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ
        found_relevant = set()
        for ret_doc in retrieved_k:
            for i, rel_doc in enumerate(relevant):
                if self._semantic_similarity(ret_doc, rel_doc) > 0.6:
                    found_relevant.add(i)

        return len(found_relevant) / len(relevant)

    def _calculate_mrr(self, retrieved: List[str], relevant: List[str]) -> float:
        """Mean Reciprocal Rank - —Å—Ä–µ–¥–Ω–µ–µ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        for rank, ret_doc in enumerate(retrieved, 1):
            for rel_doc in relevant:
                if self._semantic_similarity(ret_doc, rel_doc) > 0.6:
                    return 1.0 / rank
        return 0.0

    def _calculate_hit_rate(self, retrieved: List[str], relevant: List[str]) -> float:
        """Hit Rate - –ø—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –Ω–∞–π–¥–µ–Ω —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        for ret_doc in retrieved:
            for rel_doc in relevant:
                if self._semantic_similarity(ret_doc, rel_doc) > 0.6:
                    return 1.0
        return 0.0

    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏"""
        embeddings = self.embedding_model.encode([text1, text2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)

    async def evaluate_response_quality(self, user_id: int, test_cases: List[Dict]) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ LLM
        """
        from src.agents.RAG import RAGAgent

        rag_agent = RAGAgent()
        results = []

        for test_case in test_cases:
            query = test_case["query"]
            expected_answer = test_case.get("expected_answer", "")

            try:
                response = await asyncio.to_thread(rag_agent.run, user_id, query)

                # –û—Ü–µ–Ω–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫
                evaluation = {
                    "query": query,
                    "response": response[:200] + "..." if len(response) > 200 else response,
                    "length": len(response),
                    "relevance_score": self._calculate_relevance(query, response, expected_answer),
                    "factuality_score": self._calculate_factuality(response, expected_answer),
                    "coherence_score": self._calculate_coherence(response),
                    "contains_keywords": self._check_keywords(response, query)
                }
                results.append(evaluation)

            except Exception as e:
                results.append({
                    "query": query,
                    "error": str(e),
                    "relevance_score": 0,
                    "factuality_score": 0,
                    "coherence_score": 0
                })

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if results:
            df = pd.DataFrame(results)
            return {
                "avg_relevance": df["relevance_score"].mean(),
                "avg_factuality": df["factuality_score"].mean(),
                "avg_coherence": df["coherence_score"].mean(),
                "avg_response_length": df["length"].mean(),
                "total_queries": len(results),
                "success_rate": len([r for r in results if "error" not in r]) / len(results)
            }
        return {"error": "No results"}

    def _calculate_relevance(self, query: str, response: str, expected: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –∑–∞–ø—Ä–æ—Å—É"""
        query_embedding = self.embedding_model.encode(query)
        response_embedding = self.embedding_model.encode(response)

        similarity = cosine_similarity([query_embedding], [response_embedding])[0][0]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        query_keywords = set(query.lower().split())
        response_keywords = set(response.lower().split())
        keyword_overlap = len(query_keywords.intersection(response_keywords)) / max(len(query_keywords), 1)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        return 0.6 * similarity + 0.3 * keyword_overlap

    def _calculate_factuality(self, response: str, expected: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
        if not expected:
            return 0.5  # –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å

        response_embedding = self.embedding_model.encode(response)
        expected_embedding = self.embedding_model.encode(expected)

        similarity = cosine_similarity([response_embedding], [expected_embedding])[0][0]
        return similarity

    def _calculate_coherence(self, response: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        sentences = response.split('.')
        if len(sentences) < 2:
            return 0.3

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure_indicators = ['–≤–æ-–ø–µ—Ä–≤—ã—Ö', '–≤–æ-–≤—Ç–æ—Ä—ã—Ö', '—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã',
                                '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º']

        has_structure = any(indicator in response.lower() for indicator in structure_indicators)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ - –ø–ª–æ—Ö–æ)
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        sentence_length_score = 1.0 - abs(avg_sentence_length - 15) / 15  # –ò–¥–µ–∞–ª 15 —Å–ª–æ–≤

        return 0.3 * has_structure + 0.6 * min(1.0, sentence_length_score)

    def _check_keywords(self, response: str, query: str) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—Ç–≤–µ—Ç–µ"""
        query_words = set(word.lower() for word in query.split() if len(word) > 3)
        response_words = set(word.lower() for word in response.split() if len(word) > 3)

        return list(query_words.intersection(response_words))


async def run_evaluation(user_id: int = 12345):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    test_queries = [
        {
            "query": "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤—Ç–æ—Ä–æ–π –∑–∞–∫–æ–Ω –ù—å—é—Ç–æ–Ω–∞?",
            "relevant_docs": [
                "–í—Ç–æ—Ä–æ–π –∑–∞–∫–æ–Ω –ù—å—é—Ç–æ–Ω–∞: F = ma, —Å–∏–ª–∞ —Ä–∞–≤–Ω–∞ –º–∞—Å—Å–µ —É–º–Ω–æ–∂–µ–Ω–Ω–æ–π –Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–µ",
                "–ù—å—é—Ç–æ–Ω —É—Å—Ç–∞–Ω–æ–≤–∏–ª, —á—Ç–æ —Å–∏–ª–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—é —Ç–µ–ª–∞"
            ]
        },
        {
            "query": "–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–≤–∞–Ω—Ç–æ–≤–æ–π —Ñ–∏–∑–∏–∫–µ",
            "relevant_docs": [
                "–ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞ –∏–∑—É—á–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–∏–∫—Ä–æ—á–∞—Å—Ç–∏—Ü",
                "–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã: —Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è, –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å"
            ]
        }
    ]

    test_cases = [
        {
            "query": "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è",
            "expected_answer": "–ì—Ä–∞–≤–∏—Ç–∞—Ü–∏—è - —ç—Ç–æ —Å–∏–ª–∞ –ø—Ä–∏—Ç—è–∂–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ —Å –º–∞—Å—Å–æ–π"
        },
        {
            "query": "–ö–∞–∫–∏–µ –±—ã–≤–∞—é—Ç –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏–π?",
            "expected_answer": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ, —á–∏—Å–ª–µ–Ω–Ω—ã–µ, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏–π"
        }
    ]

    evaluator = RAGEvaluator()

    print("üîç –ù–∞—á–∏–Ω–∞—é –æ—Ü–µ–Ω–∫—É RAG —Å–∏—Å—Ç–µ–º—ã...")

    # –û—Ü–µ–Ω–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    print("\n1. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞:")
    retrieval_metrics = await evaluator.evaluate_retrieval_quality(user_id, test_queries)
    for key, value in retrieval_metrics.items():
        print(f"   {key}: {value:.3f}")

    # –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤
    print("\n2. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ LLM:")
    response_metrics = await evaluator.evaluate_response_quality(user_id, test_cases)
    for key, value in response_metrics.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")

    return {
        "retrieval_metrics": retrieval_metrics,
        "response_metrics": response_metrics
    }


if __name__ == "__main__":
    asyncio.run(run_evaluation())

