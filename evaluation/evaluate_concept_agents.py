import asyncio
from typing import Dict, List, Any
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
from sentence_transformers import SentenceTransformer
import pandas as pd


class ConceptAgentEvaluator:
    def __init__(self):
        self.embedding_model = SentenceTransformer("cointegrated/rubert-tiny2")

    async def evaluate_concept_extraction(self, test_texts: List[Dict]) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        """
        from src.agents.concept_explainer import ConceptExplainerAgent

        agent = ConceptExplainerAgent()
        results = []

        for test_case in test_texts:
            text = test_case["text"]
            expected_concepts = test_case["expected_concepts"]

            try:
                extracted_concepts = agent.extract_concepts(text, max_concepts=10)

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                extracted_names = {c["name"].lower() for c in extracted_concepts}
                expected_names = {c.lower() for c in expected_concepts}

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                precision = len(extracted_names & expected_names) / max(len(extracted_names), 1)
                recall = len(extracted_names & expected_names) / max(len(expected_names), 1)
                f1 = 2 * precision * recall / max((precision + recall), 0.001)

                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                semantic_similarity = self._calculate_concept_similarity(
                    list(extracted_names), list(expected_names)
                )

                results.append({
                    "text_id": test_case.get("id", len(results)),
                    "precision": precision,
                    "recall": recall,
                    "f1": f1,
                    "semantic_similarity": semantic_similarity,
                    "num_extracted": len(extracted_names),
                    "num_expected": len(expected_names)
                })

            except Exception as e:
                results.append({
                    "text_id": test_case.get("id", len(results)),
                    "error": str(e),
                    "precision": 0,
                    "recall": 0,
                    "f1": 0
                })

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if results:
            df = pd.DataFrame(results)
            return {
                "avg_precision": df["precision"].mean(),
                "avg_recall": df["recall"].mean(),
                "avg_f1": df["f1"].mean(),
                "avg_semantic_similarity": df["semantic_similarity"].mean(),
                "extraction_rate": len([r for r in results if r["num_extracted"] > 0]) / len(results),
                "total_tests": len(results)
            }

        return {"error": "No results"}

    def _calculate_concept_similarity(self, extracted: List[str], expected: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        if not extracted or not expected:
            return 0.0

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        all_concepts = extracted + expected
        embeddings = self.embedding_model.encode(all_concepts)

        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
        extracted_embeddings = embeddings[:len(extracted)]
        expected_embeddings = embeddings[len(extracted):]

        similarity_matrix = np.dot(extracted_embeddings, expected_embeddings.T)

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞
        max_similarities = np.max(similarity_matrix, axis=1)

        return float(np.mean(max_similarities))

    async def evaluate_concept_explanation(self, test_concepts: List[Dict]) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        """
        from src.agents.concept_explainer import ConceptExplainerAgent

        agent = ConceptExplainerAgent()
        results = []

        for test_case in test_concepts:
            concept = test_case["concept"]
            expected_explanation = test_case.get("expected_explanation", "")

            try:
                explanation_result = agent.explain_concept(concept)

                if explanation_result and "explanation" in explanation_result:
                    explanation = explanation_result["explanation"]

                    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                    clarity_score = self._evaluate_clarity(explanation)
                    completeness_score = self._evaluate_completeness(explanation, concept)
                    structure_score = self._evaluate_structure(explanation)

                    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –æ–∂–∏–¥–∞–µ–º—ã–º
                    semantic_similarity = 0.5
                    if expected_explanation:
                        semantic_similarity = self._calculate_text_similarity(
                            explanation, expected_explanation
                        )

                    results.append({
                        "concept": concept,
                        "clarity": clarity_score,
                        "completeness": completeness_score,
                        "structure": structure_score,
                        "semantic_similarity": semantic_similarity,
                        "length": len(explanation),
                        "has_examples": "examples" in explanation_result,
                        "has_key_points": "key_points" in explanation_result
                    })

            except Exception as e:
                results.append({
                    "concept": concept,
                    "error": str(e),
                    "clarity": 0,
                    "completeness": 0,
                    "structure": 0
                })

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if results:
            df = pd.DataFrame(results)
            return {
                "avg_clarity": df["clarity"].mean(),
                "avg_completeness": df["completeness"].mean(),
                "avg_structure": df["structure"].mean(),
                "avg_semantic_similarity": df["semantic_similarity"].mean(),
                "explanation_rate": len([r for r in results if "error" not in r]) / len(results),
                "examples_rate": df["has_examples"].mean() if "has_examples" in df.columns else 0,
                "key_points_rate": df["has_key_points"].mean() if "has_key_points" in df.columns else 0
            }

        return {"error": "No results"}

    def _evaluate_clarity(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —è—Å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        complex_patterns = [
            r'\b–æ–¥–Ω–∞–∫–æ\b', r'\b—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ\b', r'\b—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º\b',
            r'\b–≤–≤–∏–¥—É —Ç–æ–≥–æ —á—Ç–æ\b', r'\b–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ —á—Ç–æ\b'
        ]

        # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –ø–æ–Ω—è—Ç–Ω–µ–µ)
        sentences = text.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)

        # –û—Ü–µ–Ω–∫–∞: –º–µ–Ω—å—à–µ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —É–º–µ—Ä–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π - –ª—É—á—à–µ
        complexity_penalty = sum(1 for pattern in complex_patterns
                                 if len(re.findall(pattern, text.lower())) > 0) / len(complex_patterns)

        sentence_length_score = 1.0 - min(1.0, abs(avg_sentence_length - 15) / 15)

        return 0.6 * sentence_length_score + 0.4 * (1 - complexity_penalty)

    def _evaluate_completeness(self, explanation: str, concept: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"""
        # –ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ö–æ—Ä–æ—à–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏
        required_aspects = [
            "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–ø—Ä–∏–º–µ—Ä", "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ",
            "–ø—Ä–∏–Ω—Ü–∏–ø", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞", "–∑–Ω–∞—á–µ–Ω–∏–µ"
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç—Ç–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        explanation_lower = explanation.lower()
        aspect_coverage = sum(1 for aspect in required_aspects
                              if any(word in explanation_lower for word in aspect.split()))

        return aspect_coverage / len(required_aspects)

    def _evaluate_structure(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –ú–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure_markers = [
            '–≤–æ-–ø–µ—Ä–≤—ã—Ö', '–≤–æ-–≤—Ç–æ—Ä—ã—Ö', '–≤–æ-—Ç—Ä–µ—Ç—å–∏—Ö',
            '—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã', '—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã',
            '–Ω–∞–ø—Ä–∏–º–µ—Ä', '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ',
            '–≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å', '—Å–ª–µ–¥—É–µ—Ç –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å'
        ]

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ (–ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫)
        paragraphs = text.split('\n\n')

        # –ü–æ–¥—Å—á–µ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        markers_found = sum(1 for marker in structure_markers
                            if marker in text.lower())

        # –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure_score = min(1.0, markers_found / 3) * 0.4  # –ó–∞ –º–∞—Ä–∫–µ—Ä—ã
        paragraph_score = min(1.0, len(paragraphs) / 3) * 0.6  # –ó–∞ –∞–±–∑–∞—Ü—ã

        return structure_score + paragraph_score

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏"""
        embeddings = self.embedding_model.encode([text1, text2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return float(similarity)


async def evaluate_study_advisor():
    """–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ —É—á–µ–±–Ω—ã—Ö —Å–æ–≤–µ—Ç–æ–≤"""
    from src.agents.study_advisor import StudyAdvisorAgent

    agent = StudyAdvisorAgent()
    results = {}

    try:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–≥–µ–Ω—Ç–∞
        general_advice = agent.get_study_advice()
        notes_advice = agent.get_notes_advice("—Ñ–∏–∑–∏–∫–∞")
        memory_techniques = agent.get_memory_techniques()

        # –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤
        results["general_advice_has_structure"] = all(
            key in general_advice for key in ["advice", "quick_tips", "methods"]
        )
        results["notes_advice_has_structure"] = all(
            key in notes_advice for key in ["advice", "techniques", "tools"]
        )
        results["memory_techniques_has_structure"] = all(
            key in memory_techniques for key in ["advice", "techniques", "exercises"]
        )

        # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
        evaluator = ConceptAgentEvaluator()
        results["general_advice_clarity"] = evaluator._evaluate_clarity(
            general_advice.get("advice", "")
        )
        results["notes_advice_clarity"] = evaluator._evaluate_clarity(
            notes_advice.get("advice", "")
        )

        # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã
        results["general_advice_completeness"] = len(general_advice.get("quick_tips", [])) > 2
        results["notes_advice_completeness"] = len(notes_advice.get("techniques", [])) > 2

    except Exception as e:
        results["error"] = str(e)

    return results


async def run_concept_agent_evaluation():
    """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""

    print("üß† –ù–∞—á–∏–Ω–∞—é –æ—Ü–µ–Ω–∫—É –∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤...")

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_texts = [
        {
            "id": 1,
            "text": "–§–∏–∑–∏–∫–∞ –∏–∑—É—á–∞–µ—Ç –∑–∞–∫–æ–Ω—ã –ø—Ä–∏—Ä–æ–¥—ã. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã: –º–µ—Ö–∞–Ω–∏–∫–∞, —Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞, —ç–ª–µ–∫—Ç—Ä–æ–¥–∏–Ω–∞–º–∏–∫–∞. –ù—å—é—Ç–æ–Ω –æ—Ç–∫—Ä—ã–ª –∑–∞–∫–æ–Ω—ã –¥–≤–∏–∂–µ–Ω–∏—è.",
            "expected_concepts": ["—Ñ–∏–∑–∏–∫–∞", "–º–µ—Ö–∞–Ω–∏–∫–∞", "—Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞", "—ç–ª–µ–∫—Ç—Ä–æ–¥–∏–Ω–∞–º–∏–∫–∞", "–Ω—å—é—Ç–æ–Ω"]
        },
        {
            "id": 2,
            "text": "–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–ª–≥–µ–±—Ä—É, –≥–µ–æ–º–µ—Ç—Ä–∏—é, –∞–Ω–∞–ª–∏–∑. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∏—Å—á–∏—Å–ª–µ–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ.",
            "expected_concepts": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è", "–∞–Ω–∞–ª–∏–∑", "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∏—Å—á–∏—Å–ª–µ–Ω–∏–µ"]
        }
    ]

    test_concepts = [
        {
            "concept": "–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è",
            "expected_explanation": "–ì—Ä–∞–≤–∏—Ç–∞—Ü–∏—è - —ç—Ç–æ —Å–∏–ª–∞ –ø—Ä–∏—Ç—è–∂–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ —Å –º–∞—Å—Å–æ–π"
        },
        {
            "concept": "–∏–Ω—Ç–µ–≥—Ä–∞–ª",
            "expected_explanation": "–ò–Ω—Ç–µ–≥—Ä–∞–ª - —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –æ–±—Ä–∞—Ç–Ω–∞—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—é"
        }
    ]

    evaluator = ConceptAgentEvaluator()

    # –û—Ü–µ–Ω–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
    print("\n1. –û—Ü–µ–Ω–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤:")
    extraction_metrics = await evaluator.evaluate_concept_extraction(test_texts)
    for key, value in extraction_metrics.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")

    # –û—Ü–µ–Ω–∫–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
    print("\n2. –û—Ü–µ–Ω–∫–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤:")
    explanation_metrics = await evaluator.evaluate_concept_explanation(test_concepts)
    for key, value in explanation_metrics.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")

    # –û—Ü–µ–Ω–∫–∞ Study Advisor
    print("\n3. –û—Ü–µ–Ω–∫–∞ Study Advisor:")
    study_advisor_metrics = await evaluate_study_advisor()
    for key, value in study_advisor_metrics.items():
        print(f"   {key}: {value}")

    return {
        "concept_extraction": extraction_metrics,
        "concept_explanation": explanation_metrics,
        "study_advisor": study_advisor_metrics
    }


if __name__ == "__main__":
    asyncio.run(run_concept_agent_evaluation())
